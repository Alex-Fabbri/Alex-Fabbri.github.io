---
layout: posts
title: "A Paper a Day Keeps the Reviewer Away!"
author: Alex Fabbri
author_profile: true
comments: true
draft: true
og_image: "images/paper_image.png"
---

At least that's what I hope. In order to write better papers and familiarize myself with recent work, over the past month I made it my goal to read a new paper each day and take notes on each of them. I thought that this would be a useful way for me to better understand the papers and help others along the way. As Francis Bacon said, "some books are to be tasted, others to be swallowed, and some few to be chewed and digested," and this is reflected in my notes. Additionally, the choice of papers was very subjective, with a bias towards recent work which is relevant to my current projects. The theme of October was summarization. Let's begin! 


# 10/1 Generating Wikipedia By Summarizing Long Sequences{% cite Liu:18 --file summarization %}: 
This paper presents the first attempt to abstractively generate the first section ("lead") of Wikipedia articles through multi-document summarization of reference texts. The gold summaries are the Wikipedia leads and are fairly uniform in style due to Wikipedia's style constraints. They experiment with two types of inputs: text from the citations of a given Wikipedia page as well as searched results crawled based on the page title (up to 10 result pages, with clones removed -- motivated by articles with few citations). From a comparison of unigrams in the gold summary and input documents, they state the greater need for abstractive summarization as opposed to other text summarization datasets. Due to the large nature of the input documents when combined, emphasis is placed on the extractive step which aims to select L input tokens for the second abstractive stage. They experiment with three extractive methods: tf-idf {%cite Ramos:03 --file summarization %}, TextRank{%cite Mihalcea:04 --file summarization %} and SumBasic{%cite Nenkova:05 --file summarization %} as well as two methods of taking the first L tokens and a cheating method that takes into account bigrams in the ground truth text. They train a decoder-only Transformer{%cite Vaswani:17 --file summarization %} as they find that this allows them to train better on longer sequences of text than the encoder-decoder Transformer. This is a curious phenomenon. They evaluate on ROUGE, DUC style questions as well as human evaluations. They show the importance of a good extractor and find that the combined dataset of references plus web crawl is best. 

# 10/2 Bottom-Up Abstractive Summarization{% cite Gehrmann:18 --file summarization %}:
This paper introduces a bottom-up approach to neural summarization with an extractive content selector built upon contextual word embeddings that masks input to an abstractive summarization step. They find that their content selection model (which they pose as a sequence labeling task) is data efficient and can be trained with less than 1% of the original training data. They bring up the interesting point that in theory pointer networks should be able to do content selection by themselves. Why isn't this the case? Is there just too much input in some cases? They experiment with additional content selection methods such as masking during training, multi-task learning and masking to limit the set of possible copies during training. They try the Transformer and say it can lead to slightly improved perormance but at the cost of increased training time and parameters. However, there have been recent advances to the transformer which may alleviate this problem. They add length penalities as well as coverage penalties. They state that the *main benefit* of bottom-up summarization seems to be from the reduction of mistakenly copied words. They do an experiment in domain transfer, training a content selected on CNN data and then testing on NYT data. They do not get comparable performance, but get improvements over the non-augmented corpus. There is room for improvement in the content selector as well as in the fluency and grammaticality of the generated summaries. 


# 10/3 Beyond Generic Summarization: A Multi-faceted Hierarchical Summarization Corpus of Large Heterogeneous Data{%cite Tauchmann:18 --file summarization %}:
This paper, like {% cite Liu:18 --file summarization %}, also emphasizes that automatic summarizaton of data clusters has focused on datasets of ten to twenty ratherh short documents. They introduce an approach to create hierarchical summarization corpora from heterogeneous datasets of over 100,000 tokens from multiple genres. Their corpus focuses on topics related to education. Corpus collection is divided into two phases: 1) a crowdsourcing part extracts relevant nuggets from multiple documents 2) three expert annotators organize the nuggets into hierarchicies which are then organized into a gold standard by greedily maximizing hierarchy overlap. They release code for their annotator interface and crowdsourcing experiment to make developing summarization corpora easier. 


References
----------

{% bibliography --file summarization %}
