---
layout: posts
title: "A Paper a Day Keeps the Reviewer Away!"
author: Alex Fabbri
author_profile: true
comments: true
draft: true
og_image: "images/paper_image.png"
---


At least that's what I hope. In order to write better papers and familiarize myself with recent work, over the past month I made it my goal to read a new paper each day and take notes on each of them. I thought that this would be a useful way for me to better understand the papers and help others along the way. As Francis Bacon said, "some books are to be tasted, others to be swallowed, and some few to be chewed and digested," and this is reflected in my notes. Additionally, the choice of papers was very subjective, with a bias towards recent work which is relevant to my current projects. The theme of October was summarization. Let's begin! 

# Ideas that come up again and again in summarization papers: 
## redundancy
## news articles biased towards the first three sentences/new diverse datasets --percent of novel ngrams as a measure of abstraction needed 
## generating longer sequences of text/training on larger sequences
## evaluation metrics + human evaluation 
## other kinds of summarization (hierarchical, opinion summarization)
## datasets and standardization
## MDS and small datasets, wikisum and hierarchical summarization

# 10/1 Generating Wikipedia By Summarizing Long Sequences{% cite Liu:18 --file summarization %}: 
This paper presents the first attempt to abstractively generate the first section ("lead") of Wikipedia articles through multi-document summarization of reference texts. The gold summaries are the Wikipedia leads and are fairly uniform in style due to Wikipedia's style constraints. They experiment with two types of inputs: text from the citations of a given Wikipedia page as well as searched results crawled based on the page title (up to 10 result pages, with clones removed -- motivated by articles with few citations). From a comparison of unigrams in the gold summary and input documents, they state the greater need for abstractive summarization as opposed to other text summarization datasets. Due to the large nature of the input documents when combined, emphasis is placed on the extractive step which aims to select L input tokens for the second abstractive stage. They experiment with three extractive methods: tf-idf {%cite Ramos:03 --file summarization %}, TextRank{%cite Mihalcea:04 --file summarization %} and SumBasic{%cite Nenkova:05 --file summarization %} as well as two methods of taking the first L tokens and a cheating method that takes into account bigrams in the ground truth text. They train a decoder-only Transformer{%cite Vaswani:17 --file summarization %} as they find that this allows them to train better on longer sequences of text than the encoder-decoder Transformer. This is a curious phenomenon. They evaluate on ROUGE, DUC style questions as well as human evaluations. They show the importance of a good extractor and find that the combined dataset of references plus web crawl is best. 

# 10/2 Bottom-Up Abstractive Summarization{% cite Gehrmann:18 --file summarization %}:
This paper introduces a bottom-up approach to neural summarization with an extractive content selector built upon contextual word embeddings that masks input to an abstractive summarization step. They find that their content selection model (which they pose as a sequence labeling task) is data efficient and can be trained with less than 1% of the original training data. They bring up the interesting point that in theory pointer networks should be able to do content selection by themselves. Why isn't this the case? Is there just too much input in some cases? They experiment with additional content selection methods such as masking during training, multi-task learning and masking to limit the set of possible copies during training. They try the Transformer and say it can lead to slightly improved perormance but at the cost of increased training time and parameters. However, there have been recent advances to the transformer which may alleviate this problem. They add length penalities as well as coverage penalties. They state that the *main benefit* of bottom-up summarization seems to be from the reduction of mistakenly copied words. They do an experiment in domain transfer, training a content selected on CNN data and then testing on NYT data. They do not get comparable performance, but get improvements over the non-augmented corpus. There is room for improvement in the content selector as well as in the fluency and grammaticality of the generated summaries. 


# 10/3 Beyond Generic Summarization: A Multi-faceted Hierarchical Summarization Corpus of Large Heterogeneous Data{%cite Tauchmann:18 --file summarization %}:
This paper, like {% cite Liu:18 --file summarization %}, also emphasizes that automatic summarizaton of data clusters has focused on datasets of ten to twenty ratherh short documents. They introduce an approach to create hierarchical summarization corpora from heterogeneous datasets of over 100,000 tokens from multiple genres. Their corpus focuses on topics related to education. Corpus collection is divided into two phases: 1) a crowdsourcing part extracts relevant nuggets from multiple documents 2) three expert annotators organize the nuggets into hierarchicies which are then organized into a gold standard by greedily maximizing hierarchy overlap. They release code for their annotator interface and crowdsourcing experiment to make developing summarization corpora easier. 

# 10/4 Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised {% cite Angelidis:18 --file summarization %} 
They provide a neural frameowork for opinion summarization and introduce an opinion summarization dataset from six domains. They discuss the work as three subtasks: 1) aspect extraction or finding specific features pertaining to the entity of interest 2) sentiment prediction which determines the sentiment orientation on the aspects found in the first step and 3) summary generation which presents the opinions to the user. They take reviews and split them into segments, in this case Elemeentary Discourse Units(cite!) obtained from a Rhetorical Structure Theory parser (cite!). First aspect extraction builds upon the Aspect-based autoencoder (cite!), essentially a neural topic model. Their model improves upon this through the introduction of a set of seed words. They also introduce the oposum dataset consists of six training collections created from the Amazon Product Dataset (cite!). Really good evaluation using Best-worst scaling (cite!). 


# 10/5 A Neural Attention Model for Abstractive Sentence Summarization {% cite Rush:15 --file summarization %}
They combine a neural language model with a contextual input encoder, based off of the attention-based encoder of {% cite Bahdanau:14 --file summarization %}. They compare this encoder to a bag-of-words encoder as well as a convolutional encoder. Summaries are generating using beam search. A notable point is that the abstractive model does not have the capacity to find extractive word matches. 

# 10/6 Get To The Point: Summarization with Pointer-Generator Networks {% cite See:17 --file summarization %}
In order to improve upon abstractive summarization, this paper introduces pointer-generator networks, which is a hybrid between standard seq2seq models and pointer networks {% cite Vinyals:15 --file summarization %} which allows the model to copy words from the input. This aims to address the problem brought up in {% cite Rush:15 --file summarization %}. Additionally, repetition is a problem for sequence to sequence models. To address this, they modify the coverage model of {% cite Tu:16 --file summarization %} to penalize overlap between the attention distribution and the coverage so far. 

# 10/7 Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization {% cite Narayan:18 --file summarization %}
This paper introduces what they call "extreme summarization," which involves creating a short, one-sentence news summary answering the question of what an article is about. This task promotes multiple levels of abstraction such as paraphrasing and synthesis. They introduce a new dataset called XSum consisting of BBC articles (226,711) and accompanying single sentence summaries. While documents and summaries (the introductory sentence professionally written for each article) are shorter than other datasets, the vocabulary size is comparable to the CNN dataset. Additionally, they propose a novel abstractive model which is conditioned on the article's topics and based entirely on convolutional neural networks. 

# 10/8 Data-to-Text Generation with Content Selection and Planning {% cite Puduppully:18 --file summarization %}
Recently, end-to-end text generation has taken precendence over previous data-to-text generation which included a pipeline of content planning, sentence planning and converting this plan to output. However, neural methods do not fare well on metrics of content selection recall and factual output generation. This paper introduces an architecture of two stages 1) content selection and planning produces a content plan which specifies which records and when to introduce them in the text and 2) text generation produces the output text given the content plan as input by attending over vector representations of the records in the content plan. 

# 10/9 Graph-based neural multi-document summarizationa {% cite Yasunaga:17 --file summarization %}
This paper emphasizes that in previous neural multi-document summarizers {% cite Cao:15 --file summarization %}{%cite Cao:17 --file summarization %}, all the sentences in the same document cluster are processed independently and thus the relationships between sentences and between documents are ignored. They make use of Graph Convolutional Networks {% cite Kipf:16 --file summarization %} as well as Personalized Discourse Graphs, an extension of Approximate Discourse Graphs {% cite Christensen:13  --file summarization %} to promote diverse edge weights in a sentece relation graph. 

# 10/10 Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization {% cite Lebanoff:18 --file summarization %}

# Hierarchical Summarization: Scaling Up Multi-Document Summarization {% cite Christensen:14 --file summarization %}
They introduce a system called SUMMA which produces a hierarchy of short summaries from large document collections. Most MDS datasets contain only 10-15 documents. They say that a well-constructed hierarchical summary should maximize coverage of salient information, should minimize redundancy, should have intra-cluster coherence as well as parent-to-child coherence. Note that they use Wikipedia articles as a reference.  



References
----------


{% bibliography --file summarization %}
