



<!doctype html>
<html lang="en" class="no-js">
  <head>
    


<meta charset="utf-8">




<!-- begin SEO -->




<!----->





<title>A Paper a Day Keeps the Reviewer Away! - Alexander R. Fabbri</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Alexander R. Fabbri">
<meta property="og:title" content="A Paper a Day Keeps the Reviewer Away!">


  <link rel="canonical" href="https://Alex-Fabbri.github.io/tmp/">
  <meta property="og:url" content="https://Alex-Fabbri.github.io/tmp/">



  <meta property="og:description" content="At least that’s what I hope. In order to write better papers and familiarize myself with recent work, over the past month I made it my goal to read a new paper each day and take notes on each of them. I thought that this would be a useful way for me to better understand the papers and help others along the way. As Francis Bacon said, “some books are to be tasted, others to be swallowed, and some few to be chewed and digested,” and this is reflected in my notes. Additionally, the choice of papers was very subjective, with a bias towards recent work which is relevant to my current projects. The theme of October was summarization. Let’s begin!">





  

  



  <meta property="og:image" content="https://alex-fabbri.github.io/images/paper_image.png">



  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2018-09-13T00:00:00-04:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Alexander R. Fabbri",
      "url" : "https://Alex-Fabbri.github.io",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://Alex-Fabbri.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Alexander R. Fabbri Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://Alex-Fabbri.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

    


<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://Alex-Fabbri.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://Alex-Fabbri.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://Alex-Fabbri.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://Alex-Fabbri.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://Alex-Fabbri.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://Alex-Fabbri.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://Alex-Fabbri.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://Alex-Fabbri.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://Alex-Fabbri.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<!-- end custom head snippets -->

  </head>

   
  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    


<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://Alex-Fabbri.github.io/">Alexander R. Fabbri</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://Alex-Fabbri.github.io/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://Alex-Fabbri.github.io/year-archive/">Blog Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://Alex-Fabbri.github.io/cv/">CV</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





   



<div id="main" role="main">
  



  <div class="sidebar sticky">
  




<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://Alex-Fabbri.github.io/images/fabbri.png" class="author__avatar" alt="Alexander R. Fabbri">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Alexander R. Fabbri</h3>
    <p class="author__bio">Second-year PhD student in Computer Science at Yale University working with Professor Dragomir Radev</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
      
      
      
      
      
      
      
      
        <li><a href="https://www.linkedin.com/in/alexander-fabbri-2ab16089/"><i class="fa fa-fw fa-linkedin-square" aria-hidden="true"></i> LinkedIn</a></li>
      
      
      
      
      
      
        <li><a href="https://github.com/Alex-Fabbri"><i class="fa fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=GgfJdhwAAAAJ&hl=en"><i class="ai ai-google-scholar-square ai-fw"></i> Google Scholar</a></li>
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="A Paper a Day Keeps the Reviewer Away!">
    <meta itemprop="description" content="At least that’s what I hope. In order to write better papers and familiarize myself with recent work, over the past month I made it my goal to read a new paper each day and take notes on each of them. I thought that this would be a useful way for me to better understand the papers and help others along the way. As Francis Bacon said, “some books are to be tasted, others to be swallowed, and some few to be chewed and digested,” and this is reflected in my notes. Additionally, the choice of papers was very subjective, with a bias towards recent work which is relevant to my current projects. The theme of October was summarization. Let’s begin!">
    <meta itemprop="datePublished" content="September 13, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">A Paper a Day Keeps the Reviewer Away!
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  11 minute read
	
</p>
          
        
        
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2018-09-13T00:00:00-04:00">September 13, 2018</time></p>
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        


<!doctype html>
<html lang="en" class="no-js">
  <head>
    


<meta charset="utf-8">




<!-- begin SEO -->




<!----->





<title>A Paper a Day Keeps the Reviewer Away! - Alexander R. Fabbri</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Alexander R. Fabbri">
<meta property="og:title" content="A Paper a Day Keeps the Reviewer Away!">


  <link rel="canonical" href="https://Alex-Fabbri.github.io/tmp/">
  <meta property="og:url" content="https://Alex-Fabbri.github.io/tmp/">



  <meta property="og:description" content="At least that’s what I hope. In order to write better papers and familiarize myself with recent work, over the past month I made it my goal to read a new paper each day and take notes on each of them. I thought that this would be a useful way for me to better understand the papers and help others along the way. As Francis Bacon said, “some books are to be tasted, others to be swallowed, and some few to be chewed and digested,” and this is reflected in my notes. Additionally, the choice of papers was very subjective, with a bias towards recent work which is relevant to my current projects. The theme of October was summarization. Let’s begin!">





  

  



  <meta property="og:image" content="https://alex-fabbri.github.io/images/paper_image.png">



  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2018-09-13T00:00:00-04:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Alexander R. Fabbri",
      "url" : "https://Alex-Fabbri.github.io",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://Alex-Fabbri.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Alexander R. Fabbri Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://Alex-Fabbri.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

    


<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://Alex-Fabbri.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://Alex-Fabbri.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://Alex-Fabbri.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://Alex-Fabbri.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://Alex-Fabbri.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://Alex-Fabbri.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://Alex-Fabbri.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://Alex-Fabbri.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://Alex-Fabbri.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://Alex-Fabbri.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<!-- end custom head snippets -->

  </head>
   

  <body>

    <p>At least that’s what I hope. In order to write better papers and familiarize myself with recent work, over the past month I made it my goal to read a new paper each day and take notes on each of them. I thought that this would be a useful way for me to better understand the papers and help others along the way. As Francis Bacon said, “some books are to be tasted, others to be swallowed, and some few to be chewed and digested,” and this is reflected in my notes. Additionally, the choice of papers was very subjective, with a bias towards recent work which is relevant to my current projects. The theme of October was summarization. Let’s begin!</p>

<h1 id="ideas-that-come-up-again-and-again-in-summarization-papers">Ideas that come up again and again in summarization papers:</h1>
<h2 id="redundancy">redundancy</h2>
<h2 id="news-articles-biased-towards-the-first-three-sentencesnew-diverse-datasets-percent-of-novel-ngrams-as-a-measure-of-abstraction-needed">news articles biased towards the first three sentences/new diverse datasets –percent of novel ngrams as a measure of abstraction needed</h2>
<h2 id="generating-longer-sequences-of-texttraining-on-larger-sequences">generating longer sequences of text/training on larger sequences</h2>
<h2 id="evaluation-metrics--human-evaluation">evaluation metrics + human evaluation</h2>
<h2 id="other-kinds-of-summarization-hierarchical-opinion-summarization">other kinds of summarization (hierarchical, opinion summarization)</h2>
<h2 id="datasets-and-standardization">datasets and standardization</h2>

<h1 id="101-generating-wikipedia-by-summarizing-long-sequences1">10/1 Generating Wikipedia By Summarizing Long Sequences<a href="#Liu:18">[1]</a>:</h1>
<p>This paper presents the first attempt to abstractively generate the first section (“lead”) of Wikipedia articles through multi-document summarization of reference texts. The gold summaries are the Wikipedia leads and are fairly uniform in style due to Wikipedia’s style constraints. They experiment with two types of inputs: text from the citations of a given Wikipedia page as well as searched results crawled based on the page title (up to 10 result pages, with clones removed – motivated by articles with few citations). From a comparison of unigrams in the gold summary and input documents, they state the greater need for abstractive summarization as opposed to other text summarization datasets. Due to the large nature of the input documents when combined, emphasis is placed on the extractive step which aims to select L input tokens for the second abstractive stage. They experiment with three extractive methods: tf-idf <a href="#Ramos:03">[2]</a>, TextRank<a href="#Mihalcea:04">[3]</a> and SumBasic<a href="#Nenkova:05">[4]</a> as well as two methods of taking the first L tokens and a cheating method that takes into account bigrams in the ground truth text. They train a decoder-only Transformer<a href="#Vaswani:17">[5]</a> as they find that this allows them to train better on longer sequences of text than the encoder-decoder Transformer. This is a curious phenomenon. They evaluate on ROUGE, DUC style questions as well as human evaluations. They show the importance of a good extractor and find that the combined dataset of references plus web crawl is best.</p>

<h1 id="102-bottom-up-abstractive-summarization6">10/2 Bottom-Up Abstractive Summarization<a href="#Gehrmann:18">[6]</a>:</h1>
<p>This paper introduces a bottom-up approach to neural summarization with an extractive content selector built upon contextual word embeddings that masks input to an abstractive summarization step. They find that their content selection model (which they pose as a sequence labeling task) is data efficient and can be trained with less than 1% of the original training data. They bring up the interesting point that in theory pointer networks should be able to do content selection by themselves. Why isn’t this the case? Is there just too much input in some cases? They experiment with additional content selection methods such as masking during training, multi-task learning and masking to limit the set of possible copies during training. They try the Transformer and say it can lead to slightly improved perormance but at the cost of increased training time and parameters. However, there have been recent advances to the transformer which may alleviate this problem. They add length penalities as well as coverage penalties. They state that the <em>main benefit</em> of bottom-up summarization seems to be from the reduction of mistakenly copied words. They do an experiment in domain transfer, training a content selected on CNN data and then testing on NYT data. They do not get comparable performance, but get improvements over the non-augmented corpus. There is room for improvement in the content selector as well as in the fluency and grammaticality of the generated summaries.</p>

<h1 id="103-beyond-generic-summarization-a-multi-faceted-hierarchical-summarization-corpus-of-large-heterogeneous-data7">10/3 Beyond Generic Summarization: A Multi-faceted Hierarchical Summarization Corpus of Large Heterogeneous Data<a href="#Tauchmann:18">[7]</a>:</h1>
<p>This paper, like <a href="#Liu:18">[1]</a>, also emphasizes that automatic summarizaton of data clusters has focused on datasets of ten to twenty ratherh short documents. They introduce an approach to create hierarchical summarization corpora from heterogeneous datasets of over 100,000 tokens from multiple genres. Their corpus focuses on topics related to education. Corpus collection is divided into two phases: 1) a crowdsourcing part extracts relevant nuggets from multiple documents 2) three expert annotators organize the nuggets into hierarchicies which are then organized into a gold standard by greedily maximizing hierarchy overlap. They release code for their annotator interface and crowdsourcing experiment to make developing summarization corpora easier.</p>

<h1 id="104-summarizing-opinions-aspect-extraction-meets-sentiment-prediction-and-they-are-both-weakly-supervised-8">10/4 Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised <a href="#Angelidis:18">[8]</a></h1>
<p>They provide a neural frameowork for opinion summarization and introduce an opinion summarization dataset from six domains. They discuss the work as three subtasks: 1) aspect extraction or finding specific features pertaining to the entity of interest 2) sentiment prediction which determines the sentiment orientation on the aspects found in the first step and 3) summary generation which presents the opinions to the user. They take reviews and split them into segments, in this case Elemeentary Discourse Units(cite!) obtained from a Rhetorical Structure Theory parser (cite!). First aspect extraction builds upon the Aspect-based autoencoder (cite!), essentially a neural topic model. Their model improves upon this through the introduction of a set of seed words. They also introduce the oposum dataset consists of six training collections created from the Amazon Product Dataset (cite!). Really good evaluation using Best-worst scaling (cite!).</p>

<h1 id="105-a-neural-attention-model-for-abstractive-sentence-summarization-9">10/5 A Neural Attention Model for Abstractive Sentence Summarization <a href="#Rush:15">[9]</a></h1>
<p>They combine a neural language model with a contextual input encoder, based off of the attention-based encoder of <a href="#Bahdanau:14">[10]</a>. They compare this encoder to a bag-of-words encoder as well as a convolutional encoder. Summaries are generating using beam search. A notable point is that the abstractive model does not have the capacity to find extractive word matches.</p>

<h1 id="106-get-to-the-point-summarization-with-pointer-generator-networks-11">10/6 Get To The Point: Summarization with Pointer-Generator Networks <a href="#See:17">[11]</a></h1>
<p>In order to improve upon abstractive summarization, this paper introduces pointer-generator networks, which is a hybrid between standard seq2seq models and pointer networks <a href="#Vinyals:15">[12]</a> which allows the model to copy words from the input. This aims to address the problem brought up in <a href="#Rush:15">[9]</a>. Additionally, repetition is a problem for sequence to sequence models. To address this, they modify the coverage model of <a href="#Tu:16">[13]</a> to penalize overlap between the attention distribution and the coverage so far.</p>

<h1 id="107-dont-give-me-the-details-just-the-summary-topic-aware-convolutional-neural-networks-for-extreme-summarization-14">10/7 Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization <a href="#Narayan:18">[14]</a></h1>
<p>This paper introduces what they call “extreme summarization,” which involves creating a short, one-sentence news summary answering the question of what an article is about. This task promotes multiple levels of abstraction such as paraphrasing and synthesis. They introduce a new dataset called XSum consisting of BBC articles (226,711) and accompanying single sentence summaries. While documents and summaries (the introductory sentence professionally written for each article) are shorter than other datasets, the vocabulary size is comparable to the CNN dataset. Additionally, they propose a novel abstractive model which is conditioned on the article’s topics and based entirely on convolutional neural networks.</p>

<h1 id="108-data-to-text-generation-with-content-selection-and-planning-15">10/8 Data-to-Text Generation with Content Selection and Planning <a href="#Puduppully:18">[15]</a></h1>
<p>Recently, end-to-end text generation has taken precendence over previous data-to-text generation which included a pipeline of content planning, sentence planning and converting this plan to output. However, neural methods do not fare well on metrics of content selection recall and factual output generation. This paper introduces an architecture of two stages 1) content selection and planning produces a content plan which specifies which records and when to introduce them in the text and 2) text generation produces the output text given the content plan as input by attending over vector representations of the records in the content plan.</p>

<h1 id="109-graph-based-neural-multi-document-summarizationa-16">10/9 Graph-based neural multi-document summarizationa <a href="#Yasunaga:17">[16]</a></h1>

<h1 id="1010-adapting-the-neural-encoder-decoder-framework-from-single-to-multi-document-summarization-17">10/10 Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization <a href="#Lebanoff:18">[17]</a></h1>

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="Angelidis:18">[1]S. Angelidis, M. Lapata, Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction
               and They Are Both Weakly Supervised, CoRR. abs/1808.08858 (2018).</span></li>
<li><span id="Bahdanau:14">[2]D. Bahdanau, K. Cho, Y. Bengio, Neural Machine Translation by Jointly Learning to Align and Translate, CoRR. abs/1409.0473 (2014).</span></li>
<li><span id="Gehrmann:18">[3]S. Gehrmann, Y. Deng, A.M. Rush, Bottom-Up Abstractive Summarization, CoRR. abs/1808.10792 (2018).</span></li>
<li><span id="Lebanoff:18">[4]L. Lebanoff, K. Song, F. Liu, Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document
               Summarization, CoRR. abs/1808.06218 (2018).</span></li>
<li><span id="Liu:18">[5]P.J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, N. Shazeer, Generating Wikipedia by Summarizing Long Sequences, CoRR. abs/1801.10198 (2018).</span></li>
<li><span id="Mihalcea:04">[6]R. Mihalcea, P. Tarau, TextRank: Bringing Order into Text, in: Proceedings of the 2004 Conference on Empirical Methods in Natural
               Language Processing , EMNLP 2004, A Meeting of SIGDAT, a Special
               Interest Group of the ACL, Held in Conjunction with ACL 2004, 25-26
               July 2004, Barcelona, Spain, 2004: pp. 404–411.</span></li>
<li><span id="Narayan:18">[7]S. Narayan, S.B. Cohen, M. Lapata, Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional
               Neural Networks for Extreme Summarization, CoRR. abs/1808.08745 (2018).</span></li>
<li><span id="Nenkova:05">[8]A. Nenkova, L. Vanderwende, The impact of frequency on summarization, Microsoft Research, 2005.</span></li>
<li><span id="Puduppully:18">[9]R. Puduppully, L. Dong, M. Lapata, Data-to-Text Generation with Content Selection and Planning, (2018).</span></li>
<li><span id="Ramos:03">[10]J.E. Ramos, Using TF-IDF to Determine Word Relevance in Document Queries, in: 2003.</span></li>
<li><span id="Rush:15">[11]A.M. Rush, S. Chopra, J. Weston, A Neural Attention Model for Abstractive Sentence Summarization, in: Proceedings of the 2015 Conference on Empirical Methods in Natural
               Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21,
               2015, 2015: pp. 379–389.</span></li>
<li><span id="See:17">[12]A. See, P.J. Liu, C.D. Manning, Get To The Point: Summarization with Pointer-Generator Networks, in: Proceedings of the 55th Annual Meeting of the Association for Computational
               Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume
               1: Long Papers, 2017: pp. 1073–1083.</span></li>
<li><span id="Tauchmann:18">[13]C. Tauchmann, T. Arnold, A. Hanselowski, C.M. Meyer, M. Mieskes, Beyond Generic Summarization: A Multi-faceted Hierarchical Summarization
               Corpus of Large Heterogeneous Data, in: Proceedings of the Eleventh International Conference on Language Resources
               and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018., 2018.</span></li>
<li><span id="Tu:16">[14]Z. Tu, Z. Lu, Y. Liu, X. Liu, H. Li, Modeling Coverage for Neural Machine Translation, in: Proceedings of the 54th Annual Meeting of the Association for Computational
               Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume
               1: Long Papers, 2016.</span></li>
<li><span id="Vaswani:17">[15]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, L. Kaiser, I. Polosukhin, Attention is All you Need, in: Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, 4-9 December 2017,
               Long Beach, CA, USA, 2017: pp. 6000–6010.</span></li>
<li><span id="Vinyals:15">[16]O. Vinyals, M. Fortunato, N. Jaitly, Pointer Networks, in: Advances in Neural Information Processing Systems 28: Annual Conference
               on Neural Information Processing Systems 2015, December 7-12, 2015,
               Montreal, Quebec, Canada, 2015: pp. 2692–2700.</span></li>
<li><span id="Yasunaga:17">[17]M. Yasunaga, R. Zhang, K. Meelu, A. Pareek, K. Srinivasan, D.R. Radev, Graph-based Neural Multi-Document Summarization, in: Proceedings of the 21st Conference on Computational Natural Language
               Learning (CoNLL 2017), Vancouver, Canada, August 3-4, 2017, 2017: pp. 452–462.</span></li></ol>


    </script>

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<!-- end custom footer snippets -->

        

<div class="page__footer-copyright">&copy; 2018 Alexander R. Fabbri. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="https://Alex-Fabbri.github.io/assets/js/main.min.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119026839-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119026839-1');
</script>




  <div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://alexfabbri-1.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



  </body>
</html>

        
      </section>

      <footer class="page__meta">
        
        





      </footer>

      


<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=https://Alex-Fabbri.github.io/tmp/" class="btn btn--twitter" title="Share on Twitter"><i class="fa fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https://Alex-Fabbri.github.io/tmp/" class="btn btn--facebook" title="Share on Facebook"><i class="fa fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=https://Alex-Fabbri.github.io/tmp/" class="btn btn--google-plus" title="Share on Google Plus"><i class="fa fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://Alex-Fabbri.github.io/tmp/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fa fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>

      



  <nav class="pagination">
    
      <a href="https://Alex-Fabbri.github.io/TutorialBank/" class="pagination--pager" title="TutorialBank: Learning NLP Made Easier
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>


    </script>

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<!-- end custom footer snippets -->

        

<div class="page__footer-copyright">&copy; 2018 Alexander R. Fabbri. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="https://Alex-Fabbri.github.io/assets/js/main.min.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119026839-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119026839-1');
</script>




  </body>
</html>

